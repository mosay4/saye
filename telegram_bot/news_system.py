import feedparser
import requests
from bs4 import BeautifulSoup
from newspaper import Article
import openai
import os
from datetime import datetime, timedelta
from database import db
import schedule
import time
import threading
import logging

logger = logging.getLogger(__name__)

class NewsSystem:
    def __init__(self):
        self.db = db
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
        )
        
        # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        self.news_sources = {
            'bleeping_computer': 'https://www.bleepingcomputer.com/feed/',
            'krebs_security': 'https://krebsonsecurity.com/feed/',
            'threatpost': 'https://threatpost.com/feed/',
            'security_week': 'https://www.securityweek.com/feed',
            'dark_reading': 'https://www.darkreading.com/rss.xml',
            'cisa_alerts': 'https://www.cisa.gov/cybersecurity-advisories/all.xml'
        }
    
    def get_text(self, user_id, key):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø­Ø³Ø¨ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        user_info = self.db.get_user_info(user_id)
        lang = user_info[4] if user_info else 'ar'
        
        texts = {
            'ar': {
                'daily_news': 'ğŸ“° Ø§Ù„Ù†Ø´Ø±Ø© Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©',
                'latest_news': 'Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±',
                'news_categories': 'ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø£Ø®Ø¨Ø§Ø±',
                'critical_alerts': 'ğŸš¨ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø­Ø±Ø¬Ø©',
                'security_updates': 'ğŸ”’ ØªØ­Ø¯ÙŠØ«Ø§Øª Ø£Ù…Ù†ÙŠØ©',
                'threat_intelligence': 'ğŸ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯Ø§Øª',
                'vulnerabilities': 'ğŸ”“ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©',
                'malware_analysis': 'ğŸ¦  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ù„Ø®Ø¨ÙŠØ«Ø©',
                'no_news': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹',
                'loading_news': 'Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±...',
                'news_summary': 'Ù…Ù„Ø®Øµ Ø§Ù„Ø®Ø¨Ø±',
                'read_full': 'Ù‚Ø±Ø§Ø¡Ø© ÙƒØ§Ù…Ù„Ø©',
                'back_to_news': 'ğŸ”™ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ø£Ø®Ø¨Ø§Ø±',
                'main_menu': 'ğŸ  Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©',
                'published': 'Ù†ÙØ´Ø± ÙÙŠ:',
                'source': 'Ø§Ù„Ù…ØµØ¯Ø±:'
            },
            'en': {
                'daily_news': 'ğŸ“° Daily Newsletter',
                'latest_news': 'Latest News',
                'news_categories': 'News Categories',
                'critical_alerts': 'ğŸš¨ Critical Alerts',
                'security_updates': 'ğŸ”’ Security Updates',
                'threat_intelligence': 'ğŸ¯ Threat Intelligence',
                'vulnerabilities': 'ğŸ”“ Vulnerabilities',
                'malware_analysis': 'ğŸ¦  Malware Analysis',
                'no_news': 'No news available at the moment',
                'loading_news': 'Loading news...',
                'news_summary': 'News Summary',
                'read_full': 'Read Full',
                'back_to_news': 'ğŸ”™ Back to News',
                'main_menu': 'ğŸ  Main Menu',
                'published': 'Published:',
                'source': 'Source:'
            }
        }
        
        return texts.get(lang, texts['ar']).get(key, key)
    
    def fetch_rss_news(self, source_name, rss_url, limit=5):
        """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS"""
        try:
            feed = feedparser.parse(rss_url)
            news_items = []
            
            for entry in feed.entries[:limit]:
                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                    content = ""
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† HTML
                    if content:
                        soup = BeautifulSoup(content, 'html.parser')
                        content = soup.get_text().strip()
                    
                    # ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±
                    published_date = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_date = datetime(*entry.published_parsed[:6])
                    
                    news_item = {
                        'title': entry.title,
                        'content': content,
                        'url': entry.link,
                        'source': source_name,
                        'published_date': published_date,
                        'category': self.categorize_news(entry.title + " " + content)
                    }
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    logger.error(f"Error processing RSS entry: {e}")
                    continue
            
            return news_items
            
        except Exception as e:
            logger.error(f"Error fetching RSS from {rss_url}: {e}")
            return []
    
    def categorize_news(self, text):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['critical', 'urgent', 'emergency', 'zero-day', 'exploit']):
            return 'critical'
        elif any(word in text_lower for word in ['vulnerability', 'cve', 'patch', 'security update']):
            return 'vulnerability'
        elif any(word in text_lower for word in ['malware', 'ransomware', 'trojan', 'virus', 'botnet']):
            return 'malware'
        elif any(word in text_lower for word in ['threat', 'attack', 'breach', 'incident']):
            return 'threat'
        else:
            return 'general'
    
    def get_severity_level(self, category, content):
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©"""
        content_lower = content.lower()
        
        if category == 'critical':
            return 'critical'
        elif any(word in content_lower for word in ['widespread', 'global', 'massive', 'major']):
            return 'high'
        elif any(word in content_lower for word in ['limited', 'minor', 'small']):
            return 'low'
        else:
            return 'medium'
    
    def summarize_with_ai(self, title, content, target_language='ar'):
        """ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø®Ø¨Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            prompt = f"""
            Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø£Ù…Ù†ÙŠ Ø¨Ø§Ù„Ù„ØºØ© {target_language}:
            
            Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}
            Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {content}
            
            Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
            1. Ù…Ù„Ø®Øµ Ù…Ø®ØªØµØ± ÙÙŠ 2-3 Ø¬Ù…Ù„
            2. Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            3. Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ù…Ø­ØªÙ…Ù„
            4. Ø§Ù„ØªÙˆØµÙŠØ§Øª (Ø¥Ù† ÙˆØ¬Ø¯Øª)
            
            Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù…Ù„Ø®Øµ Ù…ÙÙ‡ÙˆÙ…Ø§Ù‹ Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ† ÙÙŠ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ.
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø£Ù…Ù†ÙŠØ©."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Error summarizing with AI: {e}")
            return content[:200] + "..." if len(content) > 200 else content
    
    def translate_content(self, content, target_language):
        """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        try:
            if target_language == 'ar':
                prompt = f"ØªØ±Ø¬Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:\n\n{content}"
            else:
                prompt = f"Translate this text to English while preserving technical terms:\n\n{content}"
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Ø£Ù†Øª Ù…ØªØ±Ø¬Ù… Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.2
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Error translating content: {e}")
            return content
    
    def collect_daily_news(self):
        """Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        all_news = []
        
        for source_name, rss_url in self.news_sources.items():
            logger.info(f"Fetching news from {source_name}")
            news_items = self.fetch_rss_news(source_name, rss_url)
            all_news.extend(news_items)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®
        all_news.sort(key=lambda x: (
            0 if x['category'] == 'critical' else 1 if x['category'] == 'vulnerability' else 2,
            -x['published_date'].timestamp()
        ))
        
        return all_news[:10]  # Ø£Ù‡Ù… 10 Ø£Ø®Ø¨Ø§Ø±
    
    def save_news_to_db(self, news_items):
        """Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        for news in news_items:
            try:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø®Ø¨Ø±
                cursor.execute('SELECT id FROM news WHERE source_url = ?', (news['url'],))
                if cursor.fetchone():
                    continue
                
                # ØªÙ„Ø®ÙŠØµ ÙˆØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                summary_ar = self.summarize_with_ai(news['title'], news['content'], 'ar')
                summary_en = self.summarize_with_ai(news['title'], news['content'], 'en')
                
                # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                title_ar = self.translate_content(news['title'], 'ar')
                title_en = news['title']  # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ØµÙ„ÙŠ Ø¹Ø§Ø¯Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
                
                severity = self.get_severity_level(news['category'], news['content'])
                
                cursor.execute('''
                    INSERT INTO news (title_ar, title_en, content_ar, content_en, 
                                    source_url, category, severity, published_date)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (title_ar, title_en, summary_ar, summary_en, 
                      news['url'], news['category'], severity, news['published_date']))
                
            except Exception as e:
                logger.error(f"Error saving news to DB: {e}")
                continue
        
        conn.commit()
        conn.close()
    
    def get_latest_news(self, user_id, limit=5, category=None):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        query = '''
            SELECT id, title_ar, title_en, content_ar, content_en, 
                   source_url, category, severity, published_date
            FROM news 
        '''
        params = []
        
        if category:
            query += ' WHERE category = ?'
            params.append(category)
        
        query += ' ORDER BY published_date DESC LIMIT ?'
        params.append(limit)
        
        cursor.execute(query, params)
        news_items = cursor.fetchall()
        conn.close()
        
        return news_items
    
    def create_news_menu(self, user_id):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
        from telegram import InlineKeyboardButton, InlineKeyboardMarkup
        
        keyboard = [
            [
                InlineKeyboardButton(self.get_text(user_id, 'latest_news'), callback_data='news_latest'),
                InlineKeyboardButton(self.get_text(user_id, 'critical_alerts'), callback_data='news_critical')
            ],
            [
                InlineKeyboardButton(self.get_text(user_id, 'vulnerabilities'), callback_data='news_vulnerability'),
                InlineKeyboardButton(self.get_text(user_id, 'malware_analysis'), callback_data='news_malware')
            ],
            [InlineKeyboardButton(self.get_text(user_id, 'main_menu'), callback_data='main_menu')]
        ]
        
        return InlineKeyboardMarkup(keyboard)
    
    def create_news_list(self, user_id, news_items):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù„Ù„Ø¹Ø±Ø¶"""
        from telegram import InlineKeyboardButton, InlineKeyboardMarkup
        
        if not news_items:
            text = self.get_text(user_id, 'no_news')
            keyboard = [[InlineKeyboardButton(self.get_text(user_id, 'back_to_news'), callback_data='news')]]
            return text, InlineKeyboardMarkup(keyboard)
        
        user_info = self.db.get_user_info(user_id)
        lang = user_info[4] if user_info else 'ar'
        
        text = f"{self.get_text(user_id, 'latest_news')}\n\n"
        keyboard = []
        
        for i, news in enumerate(news_items):
            title = news[1] if lang == 'ar' else news[2]
            severity_emoji = {
                'critical': 'ğŸš¨',
                'high': 'ğŸ”´',
                'medium': 'ğŸŸ¡',
                'low': 'ğŸŸ¢'
            }.get(news[7], 'ğŸ“°')
            
            # ØªÙ‚ØµÙŠØ± Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù„Ù„Ø¹Ø±Ø¶
            display_title = title[:50] + "..." if len(title) > 50 else title
            text += f"{severity_emoji} {display_title}\n"
            
            # Ø¥Ø¶Ø§ÙØ© Ø²Ø± Ù„Ù„Ø®Ø¨Ø±
            keyboard.append([InlineKeyboardButton(f"ğŸ“– {i+1}", callback_data=f"news_read_{news[0]}")])
        
        keyboard.append([InlineKeyboardButton(self.get_text(user_id, 'back_to_news'), callback_data='news')])
        
        return text, InlineKeyboardMarkup(keyboard)
    
    def get_news_detail(self, user_id, news_id):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø®Ø¨Ø± Ù…Ø¹ÙŠÙ†"""
        from telegram import InlineKeyboardButton, InlineKeyboardMarkup
        
        conn = self.db.get_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT title_ar, title_en, content_ar, content_en, 
                   source_url, category, severity, published_date
            FROM news WHERE id = ?
        ''', (news_id,))
        news = cursor.fetchone()
        conn.close()
        
        if not news:
            return None, None
        
        user_info = self.db.get_user_info(user_id)
        lang = user_info[4] if user_info else 'ar'
        
        title = news[0] if lang == 'ar' else news[1]
        content = news[2] if lang == 'ar' else news[3]
        
        severity_emoji = {
            'critical': 'ğŸš¨',
            'high': 'ğŸ”´',
            'medium': 'ğŸŸ¡',
            'low': 'ğŸŸ¢'
        }.get(news[6], 'ğŸ“°')
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ®
        published_date = datetime.strptime(news[7], '%Y-%m-%d %H:%M:%S')
        formatted_date = published_date.strftime('%Y-%m-%d %H:%M')
        
        text = f"{severity_emoji} {title}\n\n"
        text += f"{content}\n\n"
        text += f"ğŸ“… {self.get_text(user_id, 'published')} {formatted_date}\n"
        text += f"ğŸ”— {self.get_text(user_id, 'source')}: {news[4]}"
        
        keyboard = [
            [InlineKeyboardButton(self.get_text(user_id, 'back_to_news'), callback_data='news_latest')]
        ]
        
        return text, InlineKeyboardMarkup(keyboard)
    
    def generate_daily_newsletter(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø´Ø±Ø© Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©"""
        logger.info("Generating daily newsletter...")
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        news_items = self.collect_daily_news()
        
        if news_items:
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.save_news_to_db(news_items)
            logger.info(f"Saved {len(news_items)} news items to database")
        
        return len(news_items)
    
    def start_news_scheduler(self):
        """Ø¨Ø¯Ø¡ Ø¬Ø¯ÙˆÙ„Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
        # Ø¬Ø¯ÙˆÙ„Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª
        schedule.every(6).hours.do(self.generate_daily_newsletter)
        
        # Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø´Ø±Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© 8 Ù…Ø³Ø§Ø¡Ù‹
        schedule.every().day.at("20:00").do(self.generate_daily_newsletter)
        
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(60)  # ÙØ­Øµ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        
        logger.info("News scheduler started")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
news_system = NewsSystem()

